# Score Based Model
__(Reference)__
+ 2011(denoising score matching) https://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf
+ 2019(score based generative model) https://arxiv.org/pdf/1907.05600.pdf
+ 2021(score based SDE model) https://arxiv.org/pdf/2011.13456.pdf
+ 2021(SDEdit) https://arxiv.org/pdf/2108.01073.pdf
</br>


__(youtube)__
+ (VAE ê°•ì˜)  https://www.youtube.com/watch?v=o_peo6U7IRM
+ (ì‹œë¦½ëŒ€ ê°•ì˜) https://www.youtube.com/watch?v=HjriJyr8VZ8&list=PLeiav_J6JcY8iFItzNZ_6PMlz9W4_jz5J&index=57
+ (diffusion ê°•ì˜) https://www.youtube.com/watch?v=uFoGaIVHfoE



## Goal of Generative Model

+ ëŒ€ìƒìœ¼ë¡œ í•˜ëŠ” ë°ì´í„°ë“¤ì˜ ì‹¤ì œ ë¶„í¬ $p(x)$ê°€ ì¡´ì¬í•œë‹¤ê³  ê°€ì •.
+ ì´ë¥¼ ë°ì´í„°ë“¤ì„ ê°€ì§€ê³ , $p(x)$ì™€ $g_{\theta}(x)$ê°€ ìœ ì‚¬í•´ì§€ë„ë¡ íŒŒë¼ë¯¸í„° $\theta$ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤.
+ ì´í›„, ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•œë‹¤ëŠ” ê²ƒì€ í•™ìŠµí•œ $g_{\theta}$ì— $x_0$ë¥¼ ëŒ€ì…í•˜ì—¬  $g_{\theta}(x_0)$ ë¼ëŠ” ê²°ê³¼ë¬¼ì„ samplingí•˜ëŠ” ê²ƒì´ë‹¤.
+ ì´ë•Œ, $p(x)$ì˜ ë¶„í¬ë¥¼ ëª¨ë¥¼ ë•Œ(Implicit), ì•Œ ë•Œ(Explicit)ì˜ ê²½ìš° íŒŒë¼ë¯¸í„°ì˜ í•™ìŠµë°©ë²•ì´ ë‹¬ë¼ì§„ë‹¤.

### 1. Implicit : GAN
+ ì‚¬ì‹¤, GANì€ ë°ì´í„° ë¶„í¬í•¨ìˆ˜ $p, g$ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³ ,
+ ìƒì„±í•´ë‚¸ ë°ì´í„° ìƒ˜í”Œë“¤ì˜ ë¶„í¬, ì¦‰ $p(X), g(X)$ê°€ ë¹„ìŠ·í•´ì§€ë„ë¡ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.
    + íŒë³„ì(discriminator)ë¥¼ ì´ìš©í•´ ê²°ê³¼ë¬¼ì´ ì‹¤ì œ ë°ì´í„°ë“¤ì˜ ë¶„í¬ì— í•´ë‹¹í•˜ëŠ”ì§€ íŒê°€ë¦„í•˜ì—¬ ìƒì„±ì(generator)ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤.



### 2. Explicit
+ Explicitì´ë¼ê³ í•´ì„œ pì˜ ë¶„í¬ë¥¼ ì •í™•íˆ ì•„ëŠ”ê±´ì•„ë‹ˆê³ .. ê²°êµ­ì— ì•Œê³ ìˆëŠ” ë¶„í¬ (Gaussian ë¶„í¬)ë¡œ ê°„ì ‘ì ìœ¼ë¡œ ê·¼ì‚¬ì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤.
+ ì§„ì§œë¡œ ëª…ë°±í•˜ê²Œ ì •ì˜í•˜ëŠ” ëª¨ë¸ì€ flowëª¨ë¸??
+ ì£¼ëœ ì•„ì´ë””ì–´ëŠ”, $p_\theta(x)$ì™€ ì£¼ì–´ì§„ ìƒ˜í”Œë°ì´í„°ë“¤$(X_1, X_2, ..., X_n)$ì„ê°€ì§€ê³  ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” íŒŒë¼ë¯¸í„° $\theta$ë¥¼ ì°¾ê¸°ìœ„í•´ log-likelihoodë¥¼ ê³„ì‚°í•´, ì´ ê°’ì„ ìµœëŒ€í™”í•˜ëŠ” ê°’ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.



+ __ìµœìš°ì¶”ì •ë²•(MLE, method of Maximum Likelihood Estimation)__
+ ìš°ë„í•¨ìˆ˜ L : likelihood function
+ $f(x|\theta)$ : pdf
+ $x_1, x_2, ..., x_n$ : ê´€ì¸¡ê°’
+ $L(\theta|x_1, x_2, ..., x_n) := f(x_1|\theta)f(x_2|\theta). ..f(x_n|\theta)$

<img src="https://drive.google.com/uc?id=1aB4gJ_Mm9h2rC_x1b4LuN9wj9kkRvWG-" height=300>

$\theta^* = \arg \max \limits_{\theta} \sum_{i=1}^n(\log{p_\theta(x_i)})$
ìš”ëŸ¬í•œ íŒŒë¼ë¯¸í„° $\theta$ë¥¼ ì°¾ëŠ”ê²ƒì´ ëª©í‘œ
ë³´í†µì€ ë¯¸ë¶„í•´ì„œ 0ë˜ëŠ” ì§€ì  ì°¾ëŠ”ë‹¤.


#### 2.1 VAE, Diffusion
+ __AE(AutoEncoder)__
+ encoder : image -> latent vector ($x$-> $q_\phi(x)$)
+ decoder : latent vector -> image ($p_\theta(q_\phi(x))$)
    + ê·¸ëŸ°ë°, ì´ë ‡ê²Œí•  ê²½ìš° ìì—°ìŠ¤ëŸ½ê²Œë„ í•™ìŠµí• ë•Œ ì‚¬ìš©í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€ì—ëŒ€í•œ í‘œí˜„ë ¥ì´ ë–¨ì–´ì§„ë‹¤(discreteí•˜ê²Œ mappingí–ˆê¸° ë•Œë¬¸ì—)

+ __VAE(VariationalAutoEncoder)__
+ encoder : image -> latent vector ($x$ -> $q_\phi(z|x)$ -> $z$)
+ decoder : latent vector -> image ($z$ ->$p_\theta(x|z))$ -> $x$)
    + ì—¬ê¸°ì„œ q, pëŠ” í™•ë¥ í•¨ìˆ˜(ìœ„ì™€ êµ¬ë¶„ì§€ì–´ì•¼í•¨)
+ reparameterization trick : $\epsilon$ ~ $N(0, 1)$, $z = \mu + \sigma^2\epsilon$ -> (continuousí•˜ê²Œ mappingí•´ì¤˜ì„œ í‘œí˜„ë ¥ ì¢‹ìŒ)
+ Likelihood : $p_\theta(x) = \int{p_\theta(x|z)p_\theta(z)}dz$ ì´ê±¸ ìµœëŒ€í™” í•˜ëŠ” $\theta$ì°¾ê¸°!(by ELBO)
    + ì—¬ê¸°ì„œ p_thetaë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ì–´ì„œ ìœ„ì˜ ì ë¶„ì‹ì— q_phië¥¼ ì´ìš©í•´ ìˆ˜ì‹ ì­‰ì­‰ ì „ê°œí•´ë‚˜ê°€ì„œ ì•„ë˜ì™€ ê°™ì€ ì‹ì„ ìœ ë„í•œë‹¤.
    + __ìˆ˜ì‹ìœ ë„ ê¼­ í•´ë³´ê¸°__
<img src="https://drive.google.com/uc?id=1d4eR5ta0-gOjods4S60vOyR_IUb-oLuD" height="300">

+ __Diffusion__
+ VAEì™€ ìƒë‹¹íˆ ë¹„ìŠ·í•œ ëŠë‚Œ. VAEì—ì„œëŠ” encoderë¡œë¶€í„° ë‚˜ì˜¨ê²°ê³¼ì— ë…¸ì´ì¦ˆì¶”ê°€í–ˆë‹¤ë©´
+ ì—¬ê¸°ì„  ì´ì „time stepê³¼ noiseë¥¼ interpolationí•´ì¤Œìœ¼ë¡œì¨, ì ì€ì–‘ì˜ noiseë¥¼ time stepë§ˆë‹¤ ì¶”ê°€í•´ì¤˜ì„œ ê°€ìš°ì‹œì•ˆë¶„í¬ê¹Œì§€ í™•ì‚°(diffuse)ì‹œí‚¨ë‹¤, ë‹¤ìŒì— ê·¸ê²ƒë“¤ì„ denoisingí•´ì¤€ë‹¤. ì´ê²ƒì„ í•™ìŠµì‹œì¼œì„œ ê°€ìš°ì‹œì•ˆë¶„í¬ë¡œë¶€í„° samplingí•´ì„œ ì´ë¯¸ì§€ë¥¼ ìƒì„±
+ ë§ˆë¥´ì½”í”„ì²´ì¸ì„ ê°€ì •í•´ì„œ í™•ë¥ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³ , ê·¸ê²ƒìœ¼ë¡œ likelihoodë¥¼ ìµœëŒ€í™” í•˜ëŠ” íŒŒë¼ë¯¸í„° ê³„ì‚°

<img src="https://drive.google.com/uc?id=19lhFdBzXRkc7Z9_0wOffcPXxnfJk0ZL8">

__ìƒì„±ëª¨ë¸ë“¤ ê·¸ë¦¼__
<img src="https://drive.google.com/uc?id=19iPqI3MYLI9fg7bIAsyEwTIn9HUsCKSL" height="500">

# Score based Model

#### Energe Based Model(EBM)
+ ì–´ë–¤ xë¡œë¶€í„°ì˜ ë¶„í¬ yê°€ ìˆë‹¤ê³  í•˜ì. ex) $y=x^2$
+ ì´ë¥¼ íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ í‘œí˜„í•˜ë©´, $E_\theta(x)=x^2$ê³¼ ê°™ì´ ë³¼ ìˆ˜ ìˆë‹¤.
+ ì´ë¥¼ í™•ë¥ í•¨ìˆ˜ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤. $p_\theta(x)=\cfrac{e^{-E_\theta(x)}}{z_\theta}$, $\int{p_\theta(x)}dx = 1$

ì´ì œ, ì´ pì—ëŒ€í•´ likelihoodë¥¼ ìµœëŒ€í™”í•œë‹¤!!

#### Score Matching
+ __Score__ : $\nabla{x}\log{p(x)}$ê°’.(ë°ì´í„°ë“¤ì˜ í™•ë¥ í•¨ìˆ˜ì˜ log-likelihoodì˜ gradientê°’)
+ traditionalí•œ ë°©ë²• : $\arg\min\limits_{\theta}\mathbb{E}_{p(x)}\frac{1}{2}[||\nabla{x}
\log{p(x)} - S_\theta(x)||_2^2]$

<img src="https://drive.google.com/uc?id=1eSXNTlAvzu7y1rcli8B65MS-QMJoOihv" height="200">  
ê·¸ëŸ°ë°, pì˜ ë¶„í¬ë¥¼ ëª¨ë¥´ëŠ” ìƒí™©ì´ê¸°ë•Œë¬¸ì—  
+ $\arg\min\limits_{\theta}\mathbb{E}_{p(x)}\frac{1}{2}[||S_\theta(x)||_2^2 + tr(\nabla{x}S_\theta(x)]$ë¥¼ êµ¬í•œë‹¤. ê·¸ëŸ°ë°, ì´ë•Œ ì´ gradientì˜ ëŒ€ê°í•©ì„ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ ê° xë§ˆë‹¤ ê¸°ìš¸ê¸°ê°’ì„ ê³„ì‚°í•´ì•¼í•˜ëŠ” ë¶ˆìƒì‚¬ê°€ ë°œìƒí•œë‹¤. (ex) 100,000ê°œì˜ ë°ì´í„°ê°€ì§€ê³  í•™ìŠµí•˜ë©´ 100,000ì°¨ì›ì˜ ê¸°ìš¸ê¸°ê³„ì‚°í•´ì•¼í•¨


#### Denoising Score Matching
+ ê¸°ë³¸ idea : ì•Œê³ ìˆëŠ” ë¶„í¬ që¥¼ ì´ìš©í•´(ë³´í†µ Gaussian ë¶„í¬) xì— Noise($\sigma$)ë¥¼ ì¶”ê°€í•œ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ $q_\sigma(\tilde{x}, x)$ë¼ ì •ì˜í•˜ê³ , ì´ë†ˆì„ ì´ìš©í•´ scoreë¥¼ ì •ì˜í•œë‹¤.

$$q_\sigma(\tilde{x}, x) = q_\sigma(\tilde{x}|x)q_0(x)$$
+ $q_\sigma(\tilde{x}, x)$ : noiseê°€ ì¶”ê°€ëœ ë°ì´í„° ë¶„í¬
+ $q_\sigma(\tilde{x}|x)$ : noise ë¶„í¬
+ $q_0(x)$ : ì›ë˜ ë¶„í¬

$q_\sigma(\tilde{x}) = \int{q_\sigma(\tilde{x}, x)}dx = \int{q_\sigma(\tilde{x}|x)q_0(x)}dx \simeq \int{q_\sigma(\tilde{x}|x)p_{data}(x)}dx$  
ì´ë•Œ, $\sigma$ê°€ ì¶©ë¶„íˆ ì‘ìœ¼ë©´ ìœ„ì™€ê°™ì´ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤ê³ í•¨.
(ë‹¹ì—°íˆë„ noiseê°€ ì¶©ë¶„íˆ ì‘ë‹¤ë©´ ì›ë³¸ì´ë¯¸ì§€ì™€ ê·¸ê²Œ ë‹¤ë¥´ì§€ ì•Šì„ê²ƒì´ë¼.. diffusion ëª¨ë¸ì—ì„œ ì‘ì€ noiseì¶”ê°€í•  ë•Œ ê°€ì •í•œ ê²ƒê³¼ ê°™ì€ì›ë¦¬)

ì´ì œ, ëª©ì í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì.
$\sigma$ë¥¼ í¬ê¸°ìˆœìœ¼ë¡œ ë‚˜ì—´í•´ì„œ,
+ $\sigma_{min}=\sigma_1 < \sigma_2 < ... < \sigma_{N}=\sigma_{max}$
+ $p_{data} \simeq p_{\sigma_{min}}, p_{\sigma_i}(\tilde{x}|x) = N(\tilde{x};x, \sigma_i^2)$ìœ¼ë¡œë¶€í„°
+ $\theta^* = \arg\min\limits_{\theta} \sum_{i=1}^N{\sigma^2 \mathbb{E}_{p_{data}}\mathbb{E}_{p_{\sigma_i}(\tilde{x}, x)}[||S_\theta(\tilde{x},\sigma_i)-\nabla_{\tilde{x}}\log{p_i}(\tilde{x}|x)||_2^2]}$ë¥¼ ê³„ì‚°í•œë‹¤.
ì´ ë•Œ, $-\nabla{\tilde{x}}\log{p_{\sigma_i}}(\tilde{x}|x) \simeq \cfrac{\tilde{x}-x}{\sigma_{i}^2}$ : Gaussian Kernelë¡œ ê·¼ì‚¬í•˜ì—¬ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

#### Sampling with Langevin Dynamics
+ Langevin Dynamics : ì£¼ì–´ì§„ ë°ì´í„° xì˜ scoreë¥¼ ì•Œê³  ìˆì„ ë•Œ,
+ $z_t\sim N(0, 1), \epsilon$ì„ ì´ìš©í•´,
+ $x_t = x_{t-1} + \frac{\epsilon}{2}\nabla_x\log{p(x_{t-1})} + \sqrt{\epsilon}z_t$ì˜ ì—°ì‚°ì„ ë°˜ë³µìˆ˜í–‰í•œë‹¤.
ì´ ë•Œ, ì˜ í•™ìŠµë˜ì—ˆë‹¤ëŠ” ê°€ì •í•˜ì—
+ $x_t = x_{t-1} + \frac{\epsilon}{2}s_\theta(x_{t-1}) + \sqrt{\epsilon}z_t$ì„ í†µí•´ samplingì„ í•´ì¤€ë‹¤!!

<img src="https://drive.google.com/uc?id=15Oe7uhjLH8FcxuaH0Rc9FGMZyrP2fIsJ" height="200">

__NOTE__: Markov Chain Monte Carlo, MCMC
+ Monte Carlo : í†µê³„ë¥¼ í†µí•´ ì‹œë®¬ë ˆì´ì…˜ ìˆ˜í–‰í•˜ì—¬ ì›í•˜ëŠ” ê°’ì„ ì–»ëŠ” ê¸°ë²•
    + ì›ì£¼ìœ¨ ê³„ì‚° : ì°ëŠ” ì ì˜ ê°œìˆ˜ë¥¼ ëŠ˜ë ¤ê°€ë©° ì›ì£¼ìœ¨ ê·¼ì‚¬í•œë‹¤.
    + <img src="https://drive.google.com/uc?id=1axbZPD2S4ZrEx67eNpxivq5qqM6h5-4K">
+ Markov Chain : í˜„ì¬ stateê°€ ë°”ë¡œ ì§ì „ stateì˜ ì˜í–¥ë§Œ ë°›ëŠ” í™•ë¥  ê³¼ì •
$$p(x_t|x_{t-1}, x_{t-2}, ..., x_0) = p(x_t|x_{t_1})$$



# Score based Generative modeling through SDEs
+ (2021) scoreë¥¼ SDEë¥¼ í†µí•´ ì ‘ê·¼í•œë‹¤.


## Recall to DDPM
<img src="https://drive.google.com/uc?id=19lhFdBzXRkc7Z9_0wOffcPXxnfJk0ZL8">

### Forward Process
+ ì›ë³¸ì´ë¯¸ì§€ $x_0$ì— ë¯¸ì„¸í•œ Gaussian noiseë¥¼ hierarchicalí•˜ê²Œ ì¶”ê°€í•´ì£¼ëŠ” ê³¼ì •ì´ë‹¤.

ê° time stepì— ëŒ€í•´ì„œëŠ”
+ $q(x_t|x_{t-1}) = N(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t ğˆ)$
    + $x_{t-1}$ì— $\sqrt{1-\beta}$ë§Œí¼ ê³±í•´ë†“ê³  $\beta_t$ ë§Œí¼ì˜ ë¶„ì‚°ì„ í†µí•´ samplingí•˜ëŠ” ê²ƒì´ ê³§, $x_t$ì— noiseë¥¼ ì¶”ê°€í•œ ê²ƒì´ ëœë‹¤.
    + $\beta$ê°’ì´ ì»¤ì§ˆ ìˆ˜ë¡, noiseê°€ ì»¤ì§„ë‹¤ê³  ì´í•´í•  ìˆ˜ ìˆë‹¤.(ì—¬ê¸°ì„ , 0.0001~0.02ë¡œ ê³ ì •í•¨)

ì´ ë•Œ, ë§ˆë¥´ì½”í”„ ì—°ì‡„ì— ì˜í•´
+ $q(x_{1:T}|x_0) = \Pi_{t=1}^{T} q(x_t|x_{t-1})$
ë¥¼ ë§Œì¡±í•œë‹¤.

### Reverse Process
+ Noiseê°€ ì¶”ê°€ë˜ì–´ìˆëŠ” $x_T$ë¡œë¶€í„° denoisingì„ í•´ì£¼ëŠ” ê³¼ì •ì´ë‹¤.

ê° time stepì— ëŒ€í•´
ì „í†µ diffusion
+ $p_\theta(x_{t-1}|x_t) = N(x_{t-1};\mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$
    + ê° time tì—ëŒ€í•œ í‰ê· ê³¼ ë¶„ì‚°ì„ ì˜ˆì¸¡í•˜ê²Œë§Œë“œëŠ” íŒŒë¼ë¯¸í„° $\theta$ë¥¼ í•™ìŠµí•´ì•¼í•œë‹¤.

DDPM reverse process
+ $\Sigma_\theta(x_t, t) = \sigma_t^2I$
+ $\sigma_t^2 = \tilde{\beta_t} = \cfrac{1-\tilde{\alpha}_{t-1}}{1-\tilde{\alpha}_t} \beta_t$ or $\sigma_t^2 = \beta_t$
    + ($\beta_t$ ì— ì˜ì¡´í•˜ê¸° ë•Œë¬¸ì— ë³€ê²½)
+ $\mu_\theta(x_t, t) = \cfrac{1}{\sqrt{\bar\alpha_t}}\bigg(x_t - \cfrac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t, t)\bigg)$
    + í‰ê· ê°’$\mu$ì„ êµ¬í•˜ëŠ” ê²ƒì´ì•„ë‹ˆë¼, ì”ì°¨$\epsilon$ë¥¼ êµ¬í•˜ì! -> U-Net ì‚¬ìš©
    + ì”ì°¨ë¥¼ í†µí•´ $\mu$ë¥¼ êµ¬í•  ìˆ˜ ìˆê³ , $x_{t-1}$ì„ ìƒ˜í”Œë§í•  ìˆ˜ ìˆë‹¤.


ë§ˆë¥´ì½”í”„ ì—°ì‡„ì— ì˜í•´,
+ $p_\theta(x_{0:T}) = p(x_T)\Pi_{t=1}^T p_\theta(x_{t-1}|x_t)$

### Loss Function
ì „í†µ diffusion
+ $\mathbb{E}_q\bigg[D_{KL}(q(x_T|x_0) || p(x_T)) + \sum_{t>1}{D_{KL}(q(x_{t-1}|x_t, x_0) || p_\theta(x_{t-1}|x_t)) - \log(p_\theta(x_0|x_1))}\bigg]$

DDPM
+ $\mathbb{E}_q\bigg[\sum_{t>1}{D_{KL}(q(x_{t-1}|x_t, x_0) || p_\theta(x_{t-1}|x_t)) - \log(p_\theta(x_0|x_1))}\bigg]$
    + regularization termì„ ì œê±°í•˜ê³ , ìœ„ì—ì„œ ì •ì˜í•œ ê´€ê³„ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ë©´,

ë‹¤ìŒê³¼ê°™ì´ ë‹¨ìˆœí™” í•  ìˆ˜ ìˆë‹¤.
+ $\mathbb{E}_{t, x_0, \epsilon}\bigg[||\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon, t)||^2\bigg]$, $\epsilon \sim \mathcal{N}(0, I)$

__ìˆ˜ì‹ ìœ ë„ ì°¨ê·¼ì°¨ê·¼ ìŠ¤ìŠ¤ë¡œ í•´ë³´ê¸°!__

ì–´ì¨Œë“ , ìœ„ì˜ ìˆ˜ì‹ìœ¼ë¡œë¶€í„°, $\epsilon_\theta = s_\theta$ë¼ê³  ë³´ë©´,

samplingê³¼ì •ì„ ë‹¤ìŒê³¼ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
+ $x_{i-1} = \cfrac{1}{\sqrt{1-\beta_i}}(x_i + \beta_i s_\theta * (x_i, i)) + \sqrt{\beta_i}z_i$, $i = N, N-1, ..., 1$

## SDE ê´€ì ì—ì„œ í•´ì„
+ ì£¼ê°€ì˜ˆì¸¡ ì—°ì†ì‹œê°„ëª¨í˜•ì—ë„ ì‚¬ìš©ëœ ë°©ì •ì‹ì´ë¼ê³ í•¨..
(Ornstein-Uhlenbeck process) https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process

$$d\mathbf{x} = \mathbf{f}(\mathbf{x}, t)dt + g(t)d\mathbf{w}$$
+ $t$ : continuous time, $\{x(t)\}_{t=0}^T$ : diffusion processê°„ xê°’
+ $x(0) \sim p_0$, $x(T) \sim p_T$ : i.i.d samples


+ $\mathbf{w}$ : Standard Wiener process(_Brownian motion_)
    + tì— ëŒ€í•œ noise term, ìœ„ì˜ $\sigma$ì—­í• 
    + tì— ë”°ë¼, $w_{t+\vartriangle{t}} - w_t : \Omega \rightarrow \mathbb{R}^d$, $w_{t+\vartriangle{t}} \sim \mathcal{N}(0, \vartriangle{t})$

+ $f(\cdot, t) : \mathbb{R}^d \rightarrow \mathbb{R}^d$ : x ì— ëŒ€í•œ drift coefficient function
    + ì‹œê°„ì— ëŒ€í•œ ì¶”ì„¸(ê²½í–¥)ì„ ë°˜ì˜í–” term

+ $g(\cdot) : \mathbb{R} \rightarrow \mathbb{R}$ : $x(t)$ì— ëŒ€í•œ diffusion coefficient function
    + ì´ ë•Œ, gëŠ” xì™€ ë…ë¦½ì´ì–´ì•¼ í•œë‹¤.

ì–´ì¨Œê±°ë‚˜, ìœ„ì™€ê°™ì´ í‘œí˜„ëœ ì‹ì€ ë‹¤ìŒì˜ ì‹ìœ¼ë¡œ reverseê°€ ê°€ëŠ¥í•˜ë‹¤ê³ í•œë‹¤.
$$d\mathbf{x} = \bigg[\mathbf{f}(\mathbf{x}, t) - g(t)^2\bigtriangledown_x\log{p_t(\mathbf{x})}\bigg]dt + g(t)d\mathbf{w}$$
$0<s<t<T$, $p_{st}(x(t)|x(s))$

### SMLD(score matching langevin dynamics)

#### Forward process
$p_{\sigma_i}(\tilde{x}|x) = N(\tilde{x};x, \sigma_i^2)$ë¡œë¶€í„°,
$x_i = x_{i-1} + \sqrt{\sigma_i^2 - \sigma_{i-1}^2}z_{i-1}$ë¥¼ ëŒì–´ì˜¬ ìˆ˜ ìˆê³ , ì´ë¡œë¶€í„° ë¯¸ë¶„ë°©ì •ì‹ì„ ìœ ë„í•˜ë©´,(ì˜¤ì¼ëŸ¬ ë©”ì†Œë“œ)

$dx = \sqrt{\cfrac{d[\sigma^2(t)]}{dt}}dw$

ì´ëŠ” ìœ„ì˜ êµ¬ì¡°ì™€ ë™ì¼í•˜ë‹¤.
</br></br></br>
#### reverse process
ìˆ˜ì‹ìœ ë„ê°€.. ë‹¤ìŒì— í•´ë´ì•¼í• ë“¯

### DDPM
#### Forward process
$q(x_t|x_{t-1}) = N(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)$ë¡œë¶€í„°,
$x_t = \sqrt{1-\beta_t}x_{t-1} + \sqrt{\beta}z_{t-1}$ë¥¼ ëŒì–´ì˜¬ ìˆ˜ ìˆê³ , ì´ë¡œë¶€í„° ë¯¸ë¶„ë°©ì •ì‹ì„ ìœ ë„í•˜ë©´,(ì˜¤ì¼ëŸ¬ ë©”ì†Œë“œ)

$dx = -\cfrac{1}{2}\beta(t)xdt + \sqrt{\beta(t)}dw$
</br></br></br>

#### Reverse process
ì´ê²ƒë„ ìœ ë„ê°€... ë‹¤ìŒì—
